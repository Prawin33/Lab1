{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7360da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\prave\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\prave\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\prave\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\prave\\anaconda3\\lib\\site-packages (from python-docx) (4.7.1)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\prave\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba10de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from openpyxl import Workbook\n",
    "\n",
    "def extract_text_from_docx(docx_file):\n",
    "    doc = Document(docx_file)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def update_excel(text, excel_file):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    lines = text.split('\\n')\n",
    "    for row_index, line in enumerate(lines, start=1):\n",
    "        ws.cell(row=row_index, column=1, value=line)\n",
    "    wb.save(excel_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docx_file = \"C:/Users/Prave/Downloads/SortResume/resume.docx\"  # Path to your .docx file\n",
    "    excel_file = r'C:\\Users\\Prave\\Downloads\\SortResume\\resume.xlsx'  # Path to the Excel file you want to create/update\n",
    "\n",
    "    extracted_text = extract_text_from_docx(docx_file)\n",
    "    update_excel(extracted_text, excel_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7092c707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years of Experience: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_years_of_experience(resume_text):\n",
    "    # Regular expression to match years of experience\n",
    "    experience_regex = r\"(\\d+)\\s*(?:year|yr|years|yrs)\\s*(?:of\\s*)?(?:\\s*experience)\"\n",
    "    matches = re.findall(experience_regex, resume_text, re.IGNORECASE)\n",
    "    \n",
    "    total_years = 0\n",
    "    for match in matches:\n",
    "        total_years += int(match)\n",
    "    \n",
    "    return total_years\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resume_text = \"\"\"\n",
    "    John Doe\n",
    "    Phone: 123-456-7890\n",
    "    Email: john.doe@example.com\n",
    "\n",
    "    Summary:\n",
    "    Experienced software engineer with 5 years of experience in Python and Java development.\n",
    "\n",
    "    Education:\n",
    "    Bachelor of Science in Computer Science, XYZ University, 2015-2019\n",
    "\n",
    "    Experience:\n",
    "    Software Engineer, ABC Corp, 2019-present\n",
    "    - Developed web applications using Python and Django framework.\n",
    "    - Collaborated with cross-functional teams to deliver high-quality software solutions.\n",
    "\n",
    "    Skills:\n",
    "    - Python\n",
    "    - Java\n",
    "    - Django\n",
    "    \"\"\"\n",
    "\n",
    "    years_of_experience = extract_years_of_experience(resume_text)\n",
    "    print(\"Years of Experience:\", years_of_experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8999f533",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'C:\\Users\\Prave\\Downloads\\SortResume\\resumes\\~$esume2.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 167\u001b[0m\n\u001b[0;32m    163\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# call read text file function \u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m resume_text \u001b[38;5;241m=\u001b[39m read_docx(file_path)\n\u001b[0;32m    169\u001b[0m resume_texts\u001b[38;5;241m.\u001b[39mappend(resume_text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m    171\u001b[0m email\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extract_email_addresses(resume_text)))\n",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mread_docx\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_docx\u001b[39m(file_path):\n\u001b[1;32m---> 19\u001b[0m             doc \u001b[38;5;241m=\u001b[39m docx\u001b[38;5;241m.\u001b[39mDocument(file_path)\n\u001b[0;32m     20\u001b[0m             full_text \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\api.py:23\u001b[0m, in \u001b[0;36mDocument\u001b[1;34m(docx)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mloaded.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m docx \u001b[38;5;241m=\u001b[39m _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[1;32m---> 23\u001b[0m document_part \u001b[38;5;241m=\u001b[39m Package\u001b[38;5;241m.\u001b[39mopen(docx)\u001b[38;5;241m.\u001b[39mmain_document_part\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[0;32m     25\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\opc\\package.py:116\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     pkg_reader \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39mfrom_file(pkg_file)\n\u001b[0;32m    117\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    118\u001b[0m     Unmarshaller\u001b[38;5;241m.\u001b[39munmarshal(pkg_reader, package, PartFactory)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\opc\\pkgreader.py:22\u001b[0m, in \u001b[0;36mPackageReader.from_file\u001b[1;34m(pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(pkg_file):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     phys_reader \u001b[38;5;241m=\u001b[39m PhysPkgReader(pkg_file)\n\u001b[0;32m     23\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(phys_reader\u001b[38;5;241m.\u001b[39mcontent_types_xml)\n\u001b[0;32m     24\u001b[0m     pkg_srels \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39m_srels_for(phys_reader, PACKAGE_URI)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\opc\\phys_pkg.py:21\u001b[0m, in \u001b[0;36mPhysPkgReader.__new__\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m         reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage not found at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pkg_file)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume it's a stream and pass it to Zip reader to sort out\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: Package not found at 'C:\\Users\\Prave\\Downloads\\SortResume\\resumes\\~$esume2.docx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import subprocess  \n",
    "import docx\n",
    "import os\n",
    "import nltk\n",
    "from openpyxl import Workbook\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pyresparser import ResumeParser\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "skills_list = ['java', 'python', 'C', 'JAVASCRIPT', 'smartcomms']\n",
    "\n",
    "def read_docx(file_path):\n",
    "            doc = docx.Document(file_path)\n",
    "            full_text = []\n",
    "            for para in doc.paragraphs:\n",
    "             full_text.append(para.text)\n",
    "            return \"\\n\".join(full_text)\n",
    "\n",
    "def extract_email_addresses(text):\n",
    "    # Regular expression to match email addresses\n",
    "    email_regex = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
    "    email_addresses = re.findall(email_regex, text)\n",
    "    return email_addresses\n",
    "\n",
    "\n",
    "def extract_phone_numbers(text):\n",
    "    # Regular expression to match phone numbers\n",
    "    phone_regex = r\"\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "    phone_numbers = re.findall(phone_regex, text)\n",
    "    return phone_numbers\n",
    "\n",
    "def extract_years_of_experience(text):\n",
    "    \n",
    "    '''\n",
    "    Helper function to extract experience from resume text\n",
    "\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: list of experience\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization \n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and wordnet_lemmatizer.lemmatize(w) not in stop_words] \n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "    \n",
    "    # for i in cs.subtrees(filter=lambda x: x.label() == 'P'):\n",
    "    #     print(i)\n",
    "    \n",
    "    test = []\n",
    "    \n",
    "    for vp in list(cs.subtrees(filter=lambda x: x.label()=='P')):\n",
    "        test.append(\" \".join([i[0] for i in vp.leaves() if len(vp.leaves()) >= 2]))\n",
    "\n",
    "    # Search the word 'experience' in the chunk and then print out the text after it\n",
    "    x = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 'experience' in x.lower()]\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def extract_experience_years(resume_text):\n",
    "   \n",
    "    # Regular expression to match years of experience\n",
    "    experience_regex = r\"(\\d+)\\s*\\s*?(?:\\s*year|yr|years|yrs)\"\n",
    "    matches = re.findall(experience_regex, resume_text, re.IGNORECASE)\n",
    "    \n",
    "    total_years = 0\n",
    "    for match in matches:\n",
    "        total_years += int(match)\n",
    "    \n",
    "    return total_years\n",
    "    \n",
    "def extract_skills(resume_text, skills_list):\n",
    "    extracted_skills = []\n",
    "    for skill in skills_list:\n",
    "        if skill.lower() in resume_text.lower():\n",
    "            extracted_skills.append(skill)\n",
    "    return extracted_skills\n",
    "\n",
    "def extract_tfidf(text):\n",
    "    # Create a TF-IDF vectorizer\n",
    "    lowercase_list = [item.lower() for item in skills_list]\n",
    "    \n",
    "    tfidf_scores =[]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(vocabulary= skills_list, lowercase=True)\n",
    "\n",
    "     # Fit the vectorizer on the documents and transform them into TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform(resume_texts)\n",
    "\n",
    "     # Get feature names (terms)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "     # Convert TF-IDF matrix to DataFrame for better visualization\n",
    "    \n",
    "    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "        \n",
    "        \n",
    "    print(\"KP\",df_tfidf)\n",
    "        \n",
    "    return df_tfidf\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "email =[]\n",
    "phone =[]\n",
    "skills =[]\n",
    "experience=[]\n",
    "educations =[]\n",
    "header= [\"Email\", \"Phone\", \"Skills\", \"Experience\", \"years of Experience\", \"tfidf\"]\n",
    "expInyears =[]\n",
    "tfidf_list =[]\n",
    "resume_texts = []\n",
    "tfidf_scores=[]\n",
    "def read_resume_update_excel(resume_path, excel_path):\n",
    "   \"\"\"\n",
    "   Reads text from a resume file and updates specified fields in an existing Excel sheet.\n",
    "\n",
    "   Args:\n",
    "       resume_path (str): Path to the resume text file.\n",
    "       excel_path (str): Path to the existing Excel sheet.\n",
    "\n",
    "   Returns:\n",
    "       None\n",
    "   \"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "# Example usage:\n",
    "#file_path = \"C:/Users/Prave/Downloads/SortResume/resumes/\"\n",
    "#resume_text = read_docx(file_path)\n",
    "path = r\"C:\\Users\\Prave\\Downloads\\SortResume\\resumes\"\n",
    "  \n",
    "# Change the directory \n",
    "os.chdir(path) \n",
    "  \n",
    "# Read text File \n",
    "  \n",
    "  \n",
    "    \n",
    "  \n",
    "# iterate through all file \n",
    "for file in os.listdir(): \n",
    "    # Check whether file is in text format or not \n",
    "    if file.endswith(\".docx\"): \n",
    "        file_path = f\"{path}\\{file}\"\n",
    "  \n",
    "        # call read text file function \n",
    "        \n",
    "        resume_text = read_docx(file_path)\n",
    "        \n",
    "        resume_texts.append(resume_text.lower())\n",
    "        \n",
    "        email.append(', '.join(extract_email_addresses(resume_text)))\n",
    "        phone.append(', '.join(extract_phone_numbers(resume_text)))\n",
    "        skills.append(', '.join(extract_skills(resume_text, skills_list)))\n",
    "        experience.append(', '.join(extract_years_of_experience(resume_text)))\n",
    "        expInyears.append(extract_experience_years(resume_text))\n",
    "        #tfidf_scores.append(','.join(extract_tfidf(resume_texts)))\n",
    "          \n",
    "        #for i, scores in enumerate(tfidf_scores):\n",
    "            \n",
    "        \n",
    "        #educations.append(extract_education(resume_text))\n",
    "   \n",
    "    def write_data_to_excel(excel_file, headers, *lists):\n",
    "        \n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Write headers to the first row\n",
    "        for col, header in enumerate(headers, start=1):\n",
    "            ws.cell(row=1, column=col, value=header)\n",
    "            \n",
    "        \n",
    "        \n",
    "         # Iterate over the lists and write items to respective cells\n",
    "        for col, data_list in enumerate(lists, start=1):\n",
    "            for row, item in enumerate(data_list, start=2):\n",
    "                ws.cell(row=row, column=col, value=item)\n",
    "        \n",
    "\n",
    "        # Save the workbook to the specified Excel file\n",
    "        wb.save(excel_file)\n",
    "    \n",
    "    #skills = ', '.join(extract_skills(resume_text, skills_list))\n",
    "    #experience = extract_years_of_experience(resume_text)\n",
    "\n",
    "    #data = [  email, phone, skills, experience]\n",
    "    \n",
    "    \n",
    "    #print (email)\n",
    "    #print (skills)\n",
    "   # print (\"Test\", extract_experience_years(resume_text))\n",
    "    \n",
    "    \n",
    "    excel_file = r\"C:\\Users\\Prave\\Downloads\\SortResume\\resume.xlsx\"\n",
    "       # Save the updated DataFrame back to the Excel sheet\n",
    "    #df.to_excel(r\"C:\\Users\\Prave\\Downloads\\SortResume\\resume.xlsx\", index = False)\n",
    "    write_data_to_excel(excel_file, header, email, phone, skills, experience, expInyears)\n",
    "    #write_data_to_excel(excel_file,data)\n",
    "    df_tfidf = pd.DataFrame(extract_tfidf(resume_texts))\n",
    "    df_tfidf.to_excel(r\"C:\\Users\\Prave\\Downloads\\SortResume\\Tfidf_scores.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "       # Extract necessary information from the resume text (customize based on your needs)\n",
    "       #name = extract_name(resume_text)\n",
    "\n",
    "       \n",
    "print(\"Resume information updated successfully in Excel sheet!\")\n",
    "\n",
    "\n",
    "#except FileNotFoundError as e:\n",
    "      # print(f\"Error: File not found: {e.filename}\")\n",
    "#except Exception as e:\n",
    "      # print(f\"An error occurred: {e}\")\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4848e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document     first  third\n",
      "0  0.629228  0.777221    0.0\n",
      "1  1.000000  0.000000    0.0\n",
      "2  0.000000  0.000000    1.0\n",
      "3  0.629228  0.777221    0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Desired words for which you want to find TF-IDF scores\n",
    "desired_words = [\"document\", \"first\", \"third\"]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(vocabulary=desired_words)\n",
    "\n",
    "# Fit the vectorizer on the documents and transform them into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for better visualization\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Display TF-IDF values for desired words\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3110c776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\prave\\anaconda3\\lib\\site-packages (1.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\prave\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\prave\\anaconda3\\lib\\site-packages (from python-docx) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bdb16d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# Install SpaCy Dependencies\n",
    "os.system('python -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz')\n",
    "\n",
    "# Install nltk Dependencies\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Resume {i+1}:\")\n",
    "            print(scores)\n",
    "            print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b01a93da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "                                               Email                   Phone  \\\n",
      "2                           praveen.kale33@gmail.com  9059223961, 8499955008   \n",
      "3                          Bhindu.Daruvuri@gmail.com  4561683143, 5645464645   \n",
      "4                       Harshitha.krishnan@gmail.com  7248726876, 5758245285   \n",
      "0  jathunathan98@gmail.com, ravi@sjp.ac.lk, madus...                     NaN   \n",
      "1                           Praveen.kale33@gmail.com          (905) 922 3961   \n",
      "5                           rishikannank19@gmail.com              9790504182   \n",
      "6                      venkatesh.tarani683@gmail.com            226 386 3339   \n",
      "\n",
      "                Skills                                         Experience  \\\n",
      "2  java, C, smartcomms   Summary Overall Experience,  Ernst,  HCL Expe...   \n",
      "3  java, C, smartcomms   Summary Overall Experience,  Ernst,  HCL Expe...   \n",
      "4  java, C, smartcomms   Summary Overall Experience,  Ernst,  HCL Expe...   \n",
      "0  java, C, JAVASCRIPT                                                NaN   \n",
      "1                    C                  d Microsoft Office,  Grocer March   \n",
      "5                    C                                                NaN   \n",
      "6                    C                   Business Development Executive,    \n",
      "\n",
      "   years of Experience  tfidf     score  \n",
      "2                   12    NaN  0.000000  \n",
      "3                   12    NaN  0.000000  \n",
      "4                   13    NaN  0.000000  \n",
      "0                    0    NaN  0.197500  \n",
      "1                    0    NaN  0.435833  \n",
      "5                    8    NaN  0.435833  \n",
      "6                    0    NaN  0.435833  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "resume_details_csv = r\"C:\\Users\\Prave\\Downloads\\SortResume\\resume.xlsx\"  # Path to your CSV file\n",
    "tfidf_scores_csv = r\"C:\\Users\\Prave\\Downloads\\SortResume\\Tfidf_scores.xlsx\"\n",
    "resumes_df = pd.read_excel(resume_details_csv)\n",
    "tfidfScores_df = pd.read_excel(tfidf_scores_csv)\n",
    "#print(resume_df.head())\n",
    "\n",
    "\n",
    "feature_columns = ['java', 'python', 'JAVASCRIPT', 'smartcomms']\n",
    "\n",
    "X = tfidfScores_df.drop('C', axis=1)  # Features\n",
    "y = resumes_df['years of Experience']  # Labels\n",
    "\n",
    "# Split data into train and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(tfidf_scores_csv[feature_columns], resumes_df['years of Experience'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Rank resumes based on predicted probabilities or scores\n",
    "resumes_df['score'] = classifier.predict_proba(tfidfScores_df[feature_columns])[:, 1]\n",
    "\n",
    "# Select top 10 candidates\n",
    "top_10_resumes = resumes_df.sort_values(by='score', ascending=True).head(10)\n",
    "top_10_resumes.to_excel(r\"C:\\Users\\Prave\\Downloads\\SortResume\\SortedResumes.xlsx\", index=False)\n",
    "print(top_10_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88e73446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read CSV File\n",
    "resume_details_csv = \"\"  # Path to your CSV file\n",
    "resume_df = pd.read_csv(resume_details_csv)\n",
    "\n",
    "# Step 2: Explore the DataFrame\n",
    "# Display the first few rows\n",
    "print(resume_df.head())\n",
    "\n",
    "# Check column names\n",
    "print(resume_df.columns)\n",
    "\n",
    "# Check data types\n",
    "print(resume_df.dtypes)\n",
    "\n",
    "# Display summary statistics\n",
    "print(resume_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4534049",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m ResumeParser(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPrave\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSortResume\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mresumes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_extracted_data()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyresparser\\resume_parser.py:20\u001b[0m, in \u001b[0;36mResumeParser.__init__\u001b[1;34m(self, resume, skills_file, custom_regex)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     16\u001b[0m     resume,\n\u001b[0;32m     17\u001b[0m     skills_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m     custom_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m ):\n\u001b[1;32m---> 20\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m     custom_nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18m__file__\u001b[39m)))\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__skills_file \u001b[38;5;241m=\u001b[39m skills_file\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:30\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depr_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW001\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdepr_path), \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\util.py:175\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "data = ResumeParser(r\"C:\\Users\\Prave\\Downloads\\SortResume\\resumes\").get_extracted_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a540f07",
   "metadata": {},
   "outputs": [],
   "source": [
    " my_string = ', '.join(extract_skills(resume_text, skills_list))\n",
    "    email = extract_email_addresses(resume_text)\n",
    "    phone = ','.join(extract_phone_numbers(resume_text))\n",
    "    skills = my_string\n",
    "    experience = extract_years_of_experience(resume_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       # Update specified fields in the DataFrame\n",
    "       #df['Name'] = name\n",
    "\n",
    "    \n",
    "    \n",
    "    rows = []\n",
    "\n",
    "        # Iterate over each row of data and append it to the list\n",
    "        for row_data in data:\n",
    "            rows.append(row_data)\n",
    "\n",
    "    # Convert the list of rows to a DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Iterate over the lists and write items to respective cells\n",
    "        for col, data_list in enumerate(lists, start=1):\n",
    "            for row, item in enumerate(data_list, start=1):\n",
    "                ws.cell(row=row, column=col, value=item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
